{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name  : Diego Molina Ormazabal\n",
    "#date  : january 2021\n",
    "#mail  : diegmolina@udec.cl\n",
    "#work  : Frictional Segmentation of the Chilean Megathrust from a Multivariate Analysis of\n",
    "#        Geophysical, Geological and Geodetic Data.\n",
    "\"\"\"\n",
    "This script allows to extract the principal components (PCA) and orthogonal functions from different geophysical\n",
    "fields. The script has different steps related to the data analysis and preparation to then be used in PCA. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preparation of the script and work path\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#importation of python tools and libraries to be used\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "from scipy.io import netcdf_file as netcdf\n",
    "import netCDF4\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy import stats\n",
    "from shading import set_shade, hillshade\n",
    "import subroutine as sub\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy import interpolate\n",
    "import pandas as pd \n",
    "import plot_function_pca as pfp\n",
    "import os\n",
    "\n",
    "path_work = raw_input('Enter the path of your work directory:')\n",
    "#defining path of data to work. (grids of geophysical fields and plots)\n",
    "path_data = path_work+'/data_fields'\n",
    "path_data_grids = path_work+'/data_grids'\n",
    "path_fig_output = path_work+'/fig_outputs'\n",
    "path_grd_output = path_work+'/grid_outputs'\n",
    "\n",
    "#creating the folder to save the outputs\n",
    "path_fig_output_folder = path_work+'/fig_outputs/'\n",
    "print path_fig_output_folder\n",
    "if os.path.isdir(path_fig_output_folder):\n",
    "    print 'Directory already existes'\n",
    "if os.path.isdir(path_fig_output_folder) == False:\n",
    "    print 'Directory created'\n",
    "    os.makedirs(path_fig_output_folder)\n",
    "    \n",
    "path_fig_output_folder = path_work+'/grid_outputs/'\n",
    "print path_fig_output_folder\n",
    "if os.path.isdir(path_fig_output_folder):\n",
    "    print 'Directory already existes'\n",
    "if os.path.isdir(path_fig_output_folder) == False:\n",
    "    print 'Directory created'\n",
    "    os.makedirs(path_fig_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1 = Visualization of the geophysical fields\n",
    "\"\"\"\n",
    "print 'Step 1 == >> Loading fields, This can take a few seconds... '\n",
    "#step 1. Display of the three fields to be analised by PCA\n",
    "\n",
    "#1a) loading gravity field\n",
    "gravity = np.loadtxt(path_data+'/grav_anom_PCA_10km_400.txt')\n",
    "lon_g = gravity[:,0]; lat_g = gravity[:,1] ; grav_g = gravity[:,2] #>>extraction of lat,lon and grav_anomaly   \n",
    "#reshape of the data to the grid original grid size of Bouger Gravity anomaly\n",
    "grav_an    = np.reshape(grav_g,(3830,973))\n",
    "lon_gr     = np.reshape(lon_g,(3830,973))\n",
    "lat_gr     = np.reshape(lat_g,(3830,973))\n",
    "lon_grd    = lon_gr[0] ;     lat_grd = lat_gr[:,0] \n",
    "\n",
    "#2a) loading locking field\n",
    "#extraction lat lon lock\n",
    "lock_c        = netcdf(path_data+'/locking_chile_june15.grd','r').variables['z'][::1]\n",
    "lon_c         = netcdf(path_data+'/locking_chile_june15.grd','r').variables['x'][::1]\n",
    "lat_c         = netcdf(path_data+'/locking_chile_june15.grd','r').variables['y'][::1]\n",
    "#creation of the meshgrid to plot the data\n",
    "lon_lk , lat_lk = np.meshgrid(lon_c,lat_c)\n",
    "\n",
    "\n",
    "#3a) loading friction field\n",
    "fh = netCDF4.Dataset(path_data+'/cw_fr_chile_080.grd')\n",
    "#extraction lat lon friction\n",
    "friction  = fh.variables['Band1'][:]\n",
    "lon_f     = fh.variables['lon'][:]\n",
    "lat_f     = fh.variables['lat'][:]\n",
    "#creation of the meshgrid to plot the data\n",
    "lon_fr , lat_fr = np.meshgrid(lon_f,lat_f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#4a). Creation of the basemap definition to plot the data on map for gravity anomaly\n",
    "%matplotlib notebook\n",
    "figsize = (10, 8)\n",
    "fig = plt.figure(11,figsize = figsize)\n",
    "#loading data related to trench geometry\n",
    "fosa = np.loadtxt(path_data_grids+'/Trench.xy')\n",
    "fosa_lon = fosa[:,0]-360 ; fosa_lat = fosa[:,1]            #>> Latitud y longitud de los puntos de la fosa.\n",
    "#needed to plot trench geometry\n",
    "info_nzsa = fosa\n",
    "x = info_nzsa.T[0]; y = info_nzsa.T[1]\n",
    "x2 = x[0::12]; y2 = y[0::12]\n",
    "dx = np.diff(x2); dy = np.diff(y2)\n",
    "length = np.sqrt(dx**2 + dy**2)\n",
    "dx /= length; dy /= length\n",
    "#definition of map geographycal extension\n",
    "latmin_map = -46     #<-Latitud  min (degrees) (South)\n",
    "latmax_map = -18     #<-Latitud  max (degrees) (North)\n",
    "lonmin_map = -76     #<-Longitud min (degrees) (West)\n",
    "lonmax_map = -68     #<-Longitud max (degrees) (East)\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "\n",
    "#first subplot regarding gravity anomaly \n",
    "ax = fig.add_subplot(131)\n",
    "map = Basemap(projection='merc', resolution=\"h\", llcrnrlon=lonmin_map, llcrnrlat=latmin_map, \n",
    "              urcrnrlon=lonmax_map, urcrnrlat=latmax_map)                        \n",
    "map.drawcoastlines(zorder=21,color = 'k')\n",
    "map.drawcountries(zorder=21)\n",
    "parallels = map.drawparallels(np.linspace(-16, -46,7), labels=[1, 0, 0, 0], fmt=\"%.0f\", #4\n",
    "                dashes=[2000, 2000], zorder=24,linewidth = 0.5)\n",
    "for m in parallels:\n",
    "    try:\n",
    "        parallels[m][1][0].set_rotation(90)\n",
    "    except:\n",
    "        pass      \n",
    "map.drawmeridians([-71,-74], labels=[0, 0, 0, 1], fmt=\"%.0f\",\n",
    "               dashes=[5000, 5000],zorder=24,linewidth = 0.5)    \n",
    "\n",
    "\n",
    "map.plot(x, y, color= 'k', linewidth=1.5, zorder=41, latlon=True, ax=ax, alpha=1,clip_on=True)\n",
    "map.quiver(x2[0:20], y2[0:20], dy[0:20], -dx[0:20], latlon=True,headaxislength=8, headlength=8,headwidth=18, color= 'k', scale=40, zorder=41, ax=ax, alpha=1, clip_on=True)\n",
    "\n",
    "maxi= 150\n",
    "mini=-150\n",
    "blevels_top =np.linspace(mini,maxi,100)  \n",
    "mx = int((map.xmax-map.xmin)/1000.)+1; my = int((map.ymax-map.ymin)/1000.)+1\n",
    "\n",
    "\n",
    "anom_dat = map.transform_scalar(grav_an,lon_grd,lat_grd,mx,my, masked=True)\n",
    "intensity = hillshade(anom_dat,scale=1,azdeg=290.0,altdeg=45.0)\n",
    "\n",
    "bt1 = map.imshow(intensity, cmap='gray' ,zorder =19,alpha=1)\n",
    "bt2 = map.imshow(anom_dat,vmin= mini,vmax= maxi, cmap=plt.cm.seismic,zorder=20, alpha=0.7)\n",
    "cbar= map.colorbar(bt2,ticks= [-100,100,0],location='bottom',pad=\"6%\")\n",
    "\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "\n",
    "#second subplot regarding locking degree \n",
    "ax = fig.add_subplot(132)\n",
    "map = Basemap(projection='merc', resolution=\"h\", llcrnrlon=lonmin_map, llcrnrlat=latmin_map, \n",
    "              urcrnrlon=lonmax_map, urcrnrlat=latmax_map)                        \n",
    "map.drawcoastlines(zorder=22,color = 'k')\n",
    "map.drawcountries(zorder=22)\n",
    "parallels = map.drawparallels(np.linspace(-16, -46,7), labels=[1, 0, 0, 0], fmt=\"%.0f\", #4\n",
    "                dashes=[2000, 2000], zorder=24,linewidth = 0.5)\n",
    "for m in parallels:\n",
    "    try:\n",
    "        parallels[m][1][0].set_rotation(90)\n",
    "    except:\n",
    "        pass      \n",
    "map.drawmeridians([-71,-74], labels=[0, 0, 0, 1], fmt=\"%.0f\",\n",
    "               dashes=[5000, 5000],zorder=24,linewidth = 0.5)    \n",
    "\n",
    "\n",
    "map.plot(x, y, color= 'k', linewidth=1.5, zorder=41, latlon=True, ax=ax, alpha=1,clip_on=True)\n",
    "map.quiver(x2[0:20], y2[0:20], dy[0:20], -dx[0:20], latlon=True,headaxislength=8, headlength=8,headwidth=18, color= 'k', scale=40, zorder=41, ax=ax, alpha=1, clip_on=True)\n",
    "\n",
    "maxi= 1\n",
    "mini= 0\n",
    "blevels_top =np.linspace(mini,maxi,100)  \n",
    "\n",
    "#creationg of color palete\n",
    "mindem = 1000\n",
    "rango  = 2500\n",
    "mapu = LinearSegmentedColormap.from_list('mycmap', \n",
    "\t\t\t\t\t\t\t\t\t\t[( 0., 'white'),\n",
    "                                             ((200.+mindem)/rango, 'yellow'),\n",
    "                                             ((400.+mindem)/rango,'tomato'),\n",
    "\t\t\t\t\t\t\t\t\t    ((600.+mindem)/rango,'red'),\n",
    "\t\t\t\t\t\t\t\t\t\t((800.+mindem)/rango, 'darkred'),                                 \t\t\t\t\t\t\t\t\t\t \n",
    "\t\t\t\t\t\t\t\t\t\t(1., 'k')])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paleta   = mapu\n",
    "\n",
    "im=map.contourf(lon_lk,lat_lk,lock_c,vmin=mini,vmax=maxi,levels=blevels_top,cmap=paleta,extend='both',zorder=21,latlon = True)\n",
    "cb = map.colorbar(im,ticks= [0,0.25,0.5,0.75,1],location='bottom',pad=\"4%\") \n",
    "\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "  \n",
    "#third subplot regarding friction \n",
    "ax = fig.add_subplot(133)\n",
    "map = Basemap(projection='merc', resolution=\"h\", llcrnrlon=lonmin_map, llcrnrlat=latmin_map, \n",
    "              urcrnrlon=lonmax_map, urcrnrlat=latmax_map)                        \n",
    "map.drawcoastlines(zorder=21,color = 'k')\n",
    "map.drawcountries(zorder=21)\n",
    "parallels = map.drawparallels(np.linspace(-16, -46,7), labels=[1, 0, 0, 0], fmt=\"%.0f\", #4\n",
    "                dashes=[2000, 2000], zorder=24,linewidth = 0.5)\n",
    "for m in parallels:\n",
    "    try:\n",
    "        parallels[m][1][0].set_rotation(90)\n",
    "    except:\n",
    "        pass      \n",
    "map.drawmeridians([-71,-74], labels=[0, 0, 0, 1], fmt=\"%.0f\",\n",
    "               dashes=[5000, 5000],zorder=24,linewidth = 0.5)    \n",
    "\n",
    "\n",
    "map.plot(x, y, color= 'k', linewidth=1.5, zorder=41, latlon=True, ax=ax, alpha=1,clip_on=True)\n",
    "map.quiver(x2[0:20], y2[0:20], dy[0:20], -dx[0:20], latlon=True,headaxislength=8, headlength=8,headwidth=18, color= 'k', scale=40, zorder=41, ax=ax, alpha=1, clip_on=True)\n",
    "\n",
    "maxi= 1.1\n",
    "mini= 0.8\n",
    "blevels_top =np.linspace(mini,maxi,100)  \n",
    "\n",
    "\n",
    "im=map.contourf(lon_fr,lat_fr,friction,vmin=mini,vmax=maxi,levels=blevels_top,cmap=plt.cm.nipy_spectral,extend='both',zorder=20,latlon = True)\n",
    "cb = map.colorbar(im,ticks= [0.85,0.051,1,0],location='bottom',pad=\"4%\") \n",
    "\n",
    "\n",
    "plt.savefig(path_fig_output+'/data_fields.png',format = 'png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2 \n",
    "\n",
    "Creation of the common grid where the field where be extrapolated. Here the trench perpendicular profiles are \n",
    "built. Here it is defined the along strike spacing of the profiles and their length.\n",
    "\"\"\"\n",
    "n = int(raw_input('Enter number of perpendicular profiles (144 for this work) :'))#>> number of points or perpendicular profiles\n",
    "#2a) We obtain points from the trench homogenously spaced along strike\n",
    "#interpolation and smoothing of the trench geometry for the chilean margin along strike\n",
    "f = interpolate.interp1d(fosa_lat,fosa_lon,'linear')\n",
    "flat = np.linspace(max(fosa_lat),min(fosa_lat),10000) ; flon = f(flat)\n",
    "func = np.polyfit(flat,flon,30)\n",
    "lon_fosa_smooth = np.polyval(func,flat)\n",
    "lat_fosa_smooth = flat\n",
    "\n",
    "#2b) We divide the trench profile, for that we use a script to divide the trench in equal spaced segments for\n",
    "# which we obtain the central point on lat lon.\n",
    "\n",
    "def grilla(latmin_map, latmax_map,lonmin_map,lonmax_map, lonf,latf,lon1,lat1):\n",
    "    \n",
    "    \n",
    "#subduction zone\n",
    "    zona_subduccion    = 'Chile'\n",
    "#subduction direction \n",
    "    sentido_subduccion = 'WE'       #   EW  :  East-West  (Ej. Japon)\n",
    "                                    #   WE  :  West-East  (Ej. Chile, Ecuador_Colombia) \n",
    "        \n",
    "    fosa_lon = lon_fosa_smooth ; fosa_lat = lat_fosa_smooth \n",
    "    \n",
    "#Numero de dimensiones referente a la cantidad de componentes de los registros \n",
    "   \n",
    "    W                 = 0       #<-size fault, in this case not used.\n",
    "    nx                = n    #<-NUMBER OF POINTS OF THE TRENCH PROFILE. (perpendicular profile number)\n",
    "    ny                = 1       #<-number of elements in dip direction, 1 ==> just the trench line \n",
    "    delta_lat         = -0.08   #<-Inclination angle elements, no used\n",
    "\n",
    "    dip_plano_inicial = np.radians(5.)  #<-Dip fault, no used\n",
    "                             \n",
    "    '''\n",
    "    -------------------------------------------------------------------------------\n",
    "    Function to obtain the main length (L) of trench and strike of chilean margin. Calculated using geodetic functions\n",
    "    -------------------------------------------------------------------------------\n",
    "    '''\n",
    "\n",
    "    L, strike_aux_deg, backstrike = sub.vinc_dist(  latf,  lonf,  lat1,  lon1 )     \n",
    "\n",
    "\n",
    "    if sentido_subduccion == 'EW':\n",
    "        strike_rad = np.radians(strike_aux_deg+180)                                  #<-Strike \n",
    "    if sentido_subduccion == 'WE':\n",
    "        strike_rad = np.radians(strike_aux_deg)                                      #<-Strike \n",
    "\n",
    "    strike = strike_rad\n",
    "  \n",
    "    '''\n",
    "    -------------------------------------------------------------------------------\n",
    "    estimation of central coordinates for trench segments equaly spaced along strike for Chilean margin\n",
    "    -------------------------------------------------------------------------------\n",
    "    '''\n",
    "\n",
    "    if sentido_subduccion == 'EW':\n",
    "\n",
    "        lat0_ini,  lon0_ini,  alpha21  = sub.vinc_pt( lat1, lon1, np.degrees(strike_rad+np.pi/2), W*np.cos(dip_plano_inicial) )  \n",
    "        aux, aux, vertices_plano_inicial_lon, vertices_plano_inicial_lat =sub.coordenadas_subfallas_EW(ny,nx,dip_plano_inicial,W,L,lon0_ini,lat0_ini,strike_rad,fosa_lon,fosa_lat,delta_lat)\n",
    "    if sentido_subduccion == 'WE':\n",
    "   \n",
    "        lat0_ini,  lon0_ini,  alpha21  = sub.vinc_pt( latf, lonf, np.degrees(strike_rad+np.pi/2), W*np.cos(dip_plano_inicial) )  \n",
    "        lon_central_subfallas_slab, lat_central_subfallas_slab, vertices_plano_inicial_lon, vertices_plano_inicial_lat =sub.coordenadas_subfallas(ny,nx,dip_plano_inicial,W,L,lon0_ini,lat0_ini,strike_rad,fosa_lon,fosa_lat,delta_lat)\n",
    "\n",
    "    return np.rad2deg(strike),L,lon_central_subfallas_slab, lat_central_subfallas_slab\n",
    "    \n",
    "#3b) definiton of the portion of the trench to be used, the southern and northern  limits are defined\n",
    "\n",
    "lonf              = -73;  latf = -45        #<-southern limit trench \n",
    "lon1              = -71;  lat1 = -18.5      #<-northern limit trench \n",
    "\n",
    "#4b) Obtention of the middle points of the trench segments used to built the perpendicular profiles\n",
    "   \n",
    "St,L,lon_x, lat_x = grilla(latmin_map, latmax_map,lonmin_map,lonmax_map, lonf,latf,lon1,lat1)\n",
    "d, aux, aux = sub.vinc_dist(lat_x[0],  lon_x[0],  lat_x[1],  lon_x[1] ) \n",
    "d = d/1000.\n",
    "print 'The space between trench points is ' ,d, 'kilometers'\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "#5b) saving the lat lon points to create the profiles\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "name=path_grd_output+'/fosa_'+str(n-1)+'_'+str(int(d))+'km.txt'\n",
    "file = open(name, \"w\")\n",
    "for i in range(len(lat_x)):\n",
    "    file.write(str(lon_x[i])+' '+str(lat_x[i]))  \n",
    "    file.write('\\n')\n",
    "file.close()\n",
    "\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "#######################################################################################################################################\n",
    "\"\"\"\n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "#6b) creation of the perpendicular profiles\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "lat_tp = lat_x[::-1] ; lon_tp = lon_x[::-1] #>> Latitud y longitud de los puntos de la fosa espaciados reordenados\n",
    "\n",
    "#definition of geometrical parameters of the profiles\n",
    "\n",
    "\n",
    "largo = int(raw_input('Enter perpendicular profile length (e.x,130 km [130000]) :'))  # --->> Lenght of each profile     \n",
    "nyy = 200                      # --->> Number of points along each profile\n",
    "nx = len(lat_tp)               # --->> Number of profiles along strike\n",
    "dist_p     = largo/(nyy)       # --->> Distance between point within each profile\n",
    "p_x = 20000/int(dist_p)\n",
    "ny = nyy + p_x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#61b) Matrix to save profiles ---->creating the common grid for the fields to be interpolated\n",
    "\n",
    "\n",
    "lat_1 = np.zeros((len(lat_tp),ny+1))\n",
    "lon_1 = np.zeros((len(lon_tp),ny+1))\n",
    "\n",
    "\n",
    "'''\n",
    "--------------------------------------------------------------------------------\n",
    "62b)  Loop to calculate strike between 2 points. Then, next loop, from each point using\n",
    "      a perpendicular strike, other points are estimated using geodetic functions\n",
    "--------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "for j in range (len(lon_tp)-1):\n",
    "    \n",
    "    \n",
    "#1)  Strike of trench point\n",
    "\n",
    "    l1,Z1,z1 = sub.vinc_dist(lat_tp[j],lon_tp[j],lat_tp[j+1],lon_tp[j+1])\n",
    "    \n",
    "#1.1) Point on the trench\n",
    "             \n",
    "    P1, Q1 = lat_tp[j], lon_tp[j]\n",
    "\n",
    "#1.2) Saving first points in the matrix\n",
    "        \n",
    "    phi = P1; lmbda = Q1\n",
    "    \n",
    "    lat_1[j,0] = phi\n",
    "    lon_1[j,0] = lmbda\n",
    "          \n",
    "\n",
    "    for i in range(ny):\n",
    "        \n",
    "        #1) creating the profile and saving them\n",
    "        phi, lmbda, az = sub.vinc_pt(phi,lmbda, Z1 + 270, dist_p)\n",
    "                \n",
    "        lat_1[j,i+1] = phi\n",
    "        lon_1[j,i+1] = lmbda\n",
    "        \n",
    "\n",
    "lat_a = lat_1[:,0:nyy+1]\n",
    "lon_a = lon_1[:,0:nyy+1]\n",
    "\n",
    "\n",
    "#7b) Saving the grid points into txt file\n",
    "lat_aa = np.reshape(lat_a,(nx*(nyy+1)))\n",
    "lon_aa = np.reshape(lon_a,(nx*(nyy+1)))\n",
    "\n",
    "name=path_grd_output+'/perp_grid_'+str(int(d))+'km_'+str(n-1)+'.txt'\n",
    "file = open(name, \"w\")\n",
    "for i in range(len(lat_aa)-(nyy+1)):\n",
    "    file.write(str(lon_aa[i])+' '+str(lat_aa[i]))  \n",
    "    file.write('\\n')\n",
    "file.close()  \n",
    "\n",
    "\n",
    "\n",
    "#8b) display of the trench geometry and points, perpendicular profiles created.\n",
    "%matplotlib notebook\n",
    "figsize = (10, 8)\n",
    "fig = plt.figure(12,figsize = figsize)\n",
    "map = Basemap(projection='merc', resolution=\"h\", llcrnrlon=lonmin_map, llcrnrlat=latmin_map, \n",
    "              urcrnrlon=lonmax_map, urcrnrlat=latmax_map)                        \n",
    "map.drawcoastlines(zorder=21,color = 'k')\n",
    "map.drawcountries(zorder=21)\n",
    "parallels = map.drawparallels(np.linspace(-16, -46,7), labels=[1, 0, 0, 0], fmt=\"%.0f\", #4\n",
    "                dashes=[2000, 2000], zorder=24,linewidth = 0.5)\n",
    "for m in parallels:\n",
    "    try:\n",
    "        parallels[m][1][0].set_rotation(90)\n",
    "    except:\n",
    "        pass      \n",
    "map.drawmeridians([-71,-74], labels=[0, 0, 0, 1], fmt=\"%.0f\",\n",
    "               dashes=[5000, 5000],zorder=24,linewidth = 0.5)    \n",
    "\n",
    "map.plot(x, y, color= 'k', linewidth=0.5, zorder=41, latlon=True, alpha=1,clip_on=True)\n",
    "map.quiver(x2[0:20], y2[0:20], dy[0:20], -dx[0:20], latlon=True,headaxislength=8, headlength=8,headwidth=18, color= 'k', scale=40, zorder=41, alpha=1, clip_on=True)\n",
    "\n",
    "map.scatter(lon_x,lat_x,s = 5, marker ='.',color='r',linewidth=2,zorder = 43, latlon = True )\n",
    "for i in range(len(lat_1)):\n",
    "    map.plot(lon_a[i],lat_a[i],'k-',linewidth = 0.2, latlon=True,zorder=22)\n",
    "    \n",
    "plt.savefig(path_fig_output+'/points_ppf_trench.png',format = 'png', dpi = 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3\n",
    "\n",
    "Interpolation of the field to the new grids. The common grid is needed to work in the same space domine\n",
    "\"\"\"\n",
    "print 'Step 3 == >> Interpolating fields into the new grid, This can take a few seconds... '\n",
    "\n",
    "#definition of grid cordinates computed in the previous cell\n",
    "xi = lon_a[0:n-1,:] ; yi = lat_a[0:n-1,:]\n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "#3a) interpolating gravity field into perpendicular profiles (ppf)\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "zi_g = griddata((lon_g,lat_g),grav_g,(xi,yi),method = 'nearest', fill_value=0)\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "#3b) interpolating locking field into perpendicular profiles (ppf)\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "#reshape of the data to the interpolation\n",
    "lat_lk_grd = np.reshape(lat_lk,(len(lon_c)*len(lat_c))) ; \n",
    "lon_lk_grd = np.reshape(lon_lk,(len(lon_c)*len(lat_c))) ; \n",
    "lock_grd   = np.reshape(lock_c,(len(lon_c)*len(lat_c)))\n",
    "\n",
    "zi_l = griddata((lon_lk_grd,lat_lk_grd),lock_grd,(xi,yi),method = 'nearest', fill_value=0)\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "#3c) interpolating friction field into perpendicular profiles (ppf)\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "#reshape of the data to the interpolation\n",
    "lat_fr_grd     = np.reshape(lat_fr,(len(lon_f)*len(lat_f))) ; \n",
    "lon_fr_grd     = np.reshape(lon_fr,(len(lon_f)*len(lat_f))) ; \n",
    "friction_grd   = np.reshape(friction,(len(lon_f)*len(lat_f)))\n",
    "\n",
    "zi_f = griddata((lon_fr_grd,lat_fr_grd),friction_grd,(xi,yi),method = 'nearest', fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "#3d) map of the interpolated fields\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "%matplotlib notebook\n",
    "figsize = (10, 8)\n",
    "fig = plt.figure(13,figsize = figsize)\n",
    "plt.subplots_adjust(wspace=-0.65)\n",
    "#first subplot regarding gravity anomaly \n",
    "ax = fig.add_subplot(131)\n",
    "map = Basemap(projection='merc', resolution=\"h\", llcrnrlon=lonmin_map, llcrnrlat=latmin_map, \n",
    "              urcrnrlon=lonmax_map, urcrnrlat=latmax_map)                        \n",
    "map.drawcoastlines(zorder=22,color = 'k')\n",
    "map.drawcountries(zorder=22)\n",
    "parallels = map.drawparallels(np.linspace(-16, -46,7), labels=[1, 0, 0, 0], fmt=\"%.0f\", #4\n",
    "                dashes=[2000, 2000], zorder=24,linewidth = 0.5)\n",
    "for m in parallels:\n",
    "    try:\n",
    "        parallels[m][1][0].set_rotation(90)\n",
    "    except:\n",
    "        pass      \n",
    "map.drawmeridians([-71,-74], labels=[0, 0, 1, 0], fmt=\"%.0f\",\n",
    "               dashes=[5000, 5000],zorder=24,linewidth = 0.5,yoffset = 0.005)    \n",
    "\n",
    "\n",
    "map.plot(x, y, color= 'k', linewidth=1.5, zorder=41, latlon=True, ax=ax, alpha=1,clip_on=True)\n",
    "map.quiver(x2[0:20], y2[0:20], dy[0:20], -dx[0:20], latlon=True,headaxislength=8, headlength=8,headwidth=18, color= 'k', scale=40, zorder=41, ax=ax, alpha=1, clip_on=True)\n",
    "\n",
    "xt,yt=map(-75.8,-19)\n",
    "plt.text(xt,yt,\"a)\",color = 'k'  ,fontsize=12,zorder=41)\n",
    "\n",
    "\"\"\"\n",
    "Contornos slab\n",
    "\"\"\"\n",
    "slab_2 = np.loadtxt(path_data_grids+'/depth_south.txt')   # grillado con todas las longitudes (MxN)\n",
    "lat_sl = slab_2[:,1]\n",
    "lon_sl = slab_2[:,0] - 360 \n",
    "depth_sl = slab_2[:,2]*-1\n",
    "\n",
    "wh = np.where((depth_sl < 1000))\n",
    "depth_sl1 = depth_sl[wh]\n",
    "lon_sl1   = lon_sl[wh]\n",
    "lat_sl1   = lat_sl[wh]\n",
    "xii = np.linspace(min(lon_sl1), max(lon_sl1),1000)\n",
    "yii = np.linspace(min(lat_sl1), max(lat_sl1),1000)\n",
    "##\n",
    "xii, yii = np.meshgrid(xii, yii)\n",
    "\n",
    "zi_sl = griddata((lon_sl1,lat_sl1),depth_sl1,(xii,yii),method = 'cubic')\n",
    "edad= map.contour(xii,yii,zi_sl,levels = [50], zorder = 41, latlon = True, linewidths =2 , colors = 'k', linestyles = 'dashed')  \n",
    "\n",
    "maxi= 200\n",
    "mini=-200\n",
    "blevels_top =np.linspace(mini,maxi,100)  \n",
    "\n",
    "im=map.contourf(xi,yi,zi_g,vmin=mini,vmax=maxi,levels=blevels_top,cmap=plt.cm.seismic ,extend='neither',zorder=21,latlon = True)\n",
    "#cbar= map.colorbar(im,ticks= [-100,100,0],location='bottom', extend = 'neither',pad=\"4%\")\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "axins1 = inset_axes(ax,\n",
    "                    width=\"8%\",  # width = 50% of parent_bbox width\n",
    "                    height=\"30%\",  # height : 5%\n",
    "                    loc='lower right',\n",
    "                  )\n",
    "\n",
    "fig.colorbar(im, cax=axins1, orientation=\"vertical\", ticks=[-150,0,150],extend = 'both')\n",
    "axins1.yaxis.set_ticks_position(\"left\")\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "\n",
    "#second subplot regarding locking degree \n",
    "ax = fig.add_subplot(132)\n",
    "map = Basemap(projection='merc', resolution=\"h\", llcrnrlon=lonmin_map, llcrnrlat=latmin_map, \n",
    "              urcrnrlon=lonmax_map, urcrnrlat=latmax_map)                        \n",
    "map.drawcoastlines(zorder=22,color = 'k')\n",
    "map.drawcountries(zorder=22)\n",
    "parallels = map.drawparallels(np.linspace(-16, -46,7), labels=[0, 0, 0, 0], fmt=\"%.0f\", #4\n",
    "                dashes=[2000, 2000], zorder=24,linewidth = 0.5)\n",
    "for m in parallels:\n",
    "    try:\n",
    "        parallels[m][1][0].set_rotation(90)\n",
    "    except:\n",
    "        pass      \n",
    "map.drawmeridians([-71,-74], labels=[0, 0, 1, 0], fmt=\"%.0f\",\n",
    "               dashes=[5000, 5000],zorder=24,linewidth = 0.5,yoffset = 0.005)    \n",
    "\n",
    "\n",
    "map.plot(x, y, color= 'k', linewidth=1.5, zorder=41, latlon=True, ax=ax, alpha=1,clip_on=True)\n",
    "map.quiver(x2[0:20], y2[0:20], dy[0:20], -dx[0:20], latlon=True,headaxislength=8, headlength=8,headwidth=18, color= 'k', scale=40, zorder=41, ax=ax, alpha=1, clip_on=True)\n",
    "plt.text(xt,yt,\"b)\",color = 'k'  ,fontsize=12,zorder=41)\n",
    "zi_sl = griddata((lon_sl1,lat_sl1),depth_sl1,(xii,yii),method = 'cubic')\n",
    "edad= map.contour(xii,yii,zi_sl,levels = [50], zorder = 41, latlon = True, linewidths =2 , colors = 'k', linestyles = 'dashed')  \n",
    "\n",
    "maxi= np.nanmax(zi_l)\n",
    "mini= 0\n",
    "blevels_top =np.linspace(mini,maxi,100)  \n",
    "\n",
    "#creationg of color palete\n",
    "mindem = 1000\n",
    "rango  = 2500\n",
    "mapu = LinearSegmentedColormap.from_list('mycmap', \n",
    "\t\t\t\t\t\t\t\t\t\t[( 0., 'white'),\n",
    "                                             ((200.+mindem)/rango, 'yellow'),\n",
    "                                             ((400.+mindem)/rango,'tomato'),\n",
    "\t\t\t\t\t\t\t\t\t    ((600.+mindem)/rango,'red'),\n",
    "\t\t\t\t\t\t\t\t\t\t((800.+mindem)/rango, 'darkred'),                                 \t\t\t\t\t\t\t\t\t\t \n",
    "\t\t\t\t\t\t\t\t\t\t(1., 'k')])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "paleta   = mapu\n",
    "\n",
    "im=map.contourf(xi,yi,zi_l,vmin=mini,vmax=maxi,levels=blevels_top,cmap=paleta,extend='neither',zorder=21,latlon = True)\n",
    "#cb = map.colorbar(im,ticks= [0,0.2,0.5,0.7,1],location='bottom', extend = 'neither',pad=\"4%\") \n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "axins1 = inset_axes(ax,\n",
    "                    width=\"8%\",  # width = 50% of parent_bbox width\n",
    "                    height=\"30%\",  # height : 5%\n",
    "                    loc='lower right',\n",
    "                  )\n",
    "\n",
    "fig.colorbar(im, cax=axins1, orientation=\"vertical\", ticks=[0,0.2,0.5,0.7,1],extend = 'both')\n",
    "axins1.yaxis.set_ticks_position(\"left\")\n",
    "########################################################################################################\n",
    "########################################################################################################\n",
    "  \n",
    "#third subplot regarding friction \n",
    "ax = fig.add_subplot(133)\n",
    "map = Basemap(projection='merc', resolution=\"h\", llcrnrlon=lonmin_map, llcrnrlat=latmin_map, \n",
    "              urcrnrlon=lonmax_map, urcrnrlat=latmax_map)                        \n",
    "map.drawcoastlines(zorder=21,color = 'k')\n",
    "map.drawcountries(zorder=21)\n",
    "parallels = map.drawparallels(np.linspace(-16, -46,7), labels=[0, 1, 0, 0], fmt=\"%.0f\", #4\n",
    "                dashes=[2000, 2000], zorder=24,linewidth = 0.5)\n",
    "for m in parallels:\n",
    "    try:\n",
    "        parallels[m][1][0].set_rotation(90)\n",
    "    except:\n",
    "        pass      \n",
    "map.drawmeridians([-71,-74], labels=[0, 0, 1, 0], fmt=\"%.0f\",\n",
    "               dashes=[5000, 5000],zorder=24,linewidth = 0.5,yoffset =-10)    \n",
    "\n",
    "\n",
    "map.plot(x, y, color= 'k', linewidth=1.5, zorder=41, latlon=True, ax=ax, alpha=1,clip_on=True)\n",
    "map.quiver(x2[0:20], y2[0:20], dy[0:20], -dx[0:20], latlon=True,headaxislength=8, headlength=8,headwidth=18, color= 'k', scale=40, zorder=41, ax=ax, alpha=1, clip_on=True)\n",
    "plt.text(xt,yt,\"c)\",color = 'k'  ,fontsize=12,zorder=41)\n",
    "zi_sl = griddata((lon_sl1,lat_sl1),depth_sl1,(xii,yii),method = 'cubic')\n",
    "edad= map.contour(xii,yii,zi_sl,levels = [50], zorder = 41, latlon = True, linewidths =2 , colors = 'k', linestyles = 'dashed')  \n",
    "\n",
    "maxi= 1.1\n",
    "mini= 0.6\n",
    "blevels_top =np.linspace(mini,maxi,100)  \n",
    "\n",
    "\n",
    "im=map.contourf(xi,yi,zi_f,vmin=mini,vmax=maxi,levels=blevels_top,cmap=plt.cm.nipy_spectral,extend='neither',zorder=20,latlon = True)\n",
    "#cb = map.colorbar(im,ticks= [0.85,0.051,1,0],location='bottom', extend = 'neither',pad=\"4%\") \n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "axins1 = inset_axes(ax,\n",
    "                    width=\"8%\",  # width = 50% of parent_bbox width\n",
    "                    height=\"30%\",  # height : 5%\n",
    "                    loc='lower right',\n",
    "                  )\n",
    "\n",
    "fig.colorbar(im, cax=axins1, orientation=\"vertical\", ticks=[0,0.85,1],extend = 'both')\n",
    "axins1.yaxis.set_ticks_position(\"left\")\n",
    "\n",
    "plt.savefig(path_fig_output+'/data_fields_interpolated.png',format = 'png', dpi = 300)\n",
    "plt.savefig(path_fig_output+'/data_fields_interpolated.svg',format = 'svg', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 4\n",
    "\n",
    "PCA implementation. Here there are different cases regarding to the number of fields to be analized \n",
    "and to the modes to be extracted\n",
    "\n",
    "#if the PCA is performed just to one field (c=1), the standarization of the data it is not needed. However \n",
    "#if the number of fields is bigger than 2, the standarization must be applied in order to avoid prevalence of \n",
    "#a field over other just because the amplitudes. \n",
    "\"\"\"\n",
    "\n",
    "print 'Step 4 == >> PCA implementation, preparing the data > standarization: This can take a few seconds... '\n",
    "\n",
    "                             \n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "4a) Standarization data. Definition of how many and what fields will be analized (C=i). Definition of which mode\n",
    "    will be extracted (K=i). Profiles into matrix\n",
    "    To do this, first the data need no be standarized by removing mean field and dividing by standar deviation\n",
    "    When the field is analized by itself, does not need to be standarized\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "nx = len(xi) ; ny = len(yi[0])\n",
    "\n",
    "z_grav = zi_g; z_lock_0 = zi_l; z_frt = zi_f\n",
    "\n",
    "\n",
    "\n",
    "# Note: Gravity mean does not need to be removed because it is already an anomaly\n",
    "\n",
    "C_G = np.zeros((nx,ny))                           ####-----------> matrix containing gravity \n",
    "C_Gn = np.zeros((nx,ny))                          ####-----------> matrix containing gravity no-nstandarized\n",
    "\n",
    "#4.1a) Normalization of gravity profiles using standard deviation of the field               \n",
    "for M in range(nx):\n",
    "    for N in range(ny):\n",
    "        C_G[M,N]  = (z_grav[M,N]  ) /np.nanstd(z_grav)  \n",
    "        C_Gn[M,N] = z_grav[M,N]/np.nanstd(z_grav) \n",
    "        \n",
    "#4.2a) Normalization of locking profiles using standard deviation of the field               \n",
    "C_L = np.zeros((nx,ny))                           ####-----------> matrix containing locking\n",
    "C_Ln = np.zeros((nx,ny))                          ####-----------> matrix containing locking no-nstandarized\n",
    "\n",
    "for M in range(nx):\n",
    "    for N in range(ny):\n",
    "        C_L[M,N]  = (z_lock_0[M,N] - np.nanmean(z_lock_0)) /np.nanstd(z_lock_0)  \n",
    "        C_Ln[M,N] =  z_lock_0[M,N]/np.nanstd(z_lock_0)\n",
    "#4.3a) Normalization of friction profiles using standard deviation of the field    \n",
    "C_F = np.zeros((nx,ny))                           ####-----------> matrix containing friction\n",
    "C_Fn = np.zeros((nx,ny))                          ####-----------> matrix containing friction no-nstandarized\n",
    "\n",
    "for M in range(nx):\n",
    "    for N in range(ny):\n",
    "        C_F[M,N]  = (z_frt[M,N] - np.nanmean(z_frt)) /np.nanstd(z_frt)  \n",
    "        C_Fn[M,N] = z_frt[M,N]/np.nanstd(z_frt) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "4b) Buiding co-variance matrix to extract the singular values. (R)\n",
    "    Here several cases can be defined\n",
    "    To analize an specific fields, define the name as a number; gravity = 1, locking = 2, friction = 3 \n",
    "    To extract an specific mode, define the mode   k = 1, 2, 3 .... n\n",
    "    to analize 1 ,2 or 3 fields, define the number c = 1, 2 or 3\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "print 'To define the number of fields, enter C as 1, 2 or 3'\n",
    "c           = float(raw_input('Number of fields to analize :'))\n",
    "print 'To define the fields to be analized, enter name of fields as gravity = 1, locking = 2 or friction = 3'\n",
    "print 'example --->  1,3'\n",
    "name_fields = eval(raw_input('Field number:'))\n",
    "print 'To define the mode to be extracted, enter k as 1, 2,3, ..., ny'\n",
    "k           = float(raw_input('Mode number :'))\n",
    "k = int(k)\n",
    "#41b) Case to apply PCA just to one field\n",
    "if  c == 1:\n",
    "\n",
    "    if name_fields == 1:\n",
    "\n",
    "        D = np.vstack((C_G))           ###------->>  Concatenated matrix of gravity\n",
    "    if name_fields == 2:\n",
    "\n",
    "        D = np.vstack((C_L))           ###------->>  Concatenated matrix of locking\n",
    "    if name_fields == 3:\n",
    "\n",
    "        D = np.vstack((C_F))           ###------->>  Concatenated matrix of friction\n",
    "    \n",
    "    name_fields = np.array([name_fields])\n",
    "#41b) Case to apply PCA just to two fields\n",
    "if  c == 2:\n",
    "\n",
    "    if sum(name_fields) == 3:\n",
    "\n",
    "        D = np.vstack((C_G,C_L))        ###------->> Concatenated matrix of gravity-locking\n",
    "    if sum(name_fields) == 4:\n",
    "\n",
    "        D = np.vstack((C_G,C_F))        ###------->> Concatenated matrix of gravity-friction\n",
    "    if sum(name_fields) == 5:           \n",
    "\n",
    "        D = np.vstack((C_L,C_F))        ###------->> Concatenated matrix of locking-friction\n",
    "    \n",
    "    name_fields = np.array(name_fields)\n",
    "   \n",
    "        \n",
    "#41c) Case to apply PCA to three fields\n",
    "if  c == 3:\n",
    "\n",
    "        D = np.vstack((C_G,C_L,C_F))    ###------->> Concatenated matrix of gravity-locking-friction\n",
    "\n",
    "        name_fields = np.array(name_fields)\n",
    "\n",
    "#4c) ----->> Covariance matrix of D and extraction of eigen values and eigen vectors\n",
    "#note that nans are replaced by zero values, so these points do not contribute to the \n",
    "#final covariance\n",
    "where_are_NaNs = np.isnan(D)\n",
    "D[where_are_NaNs] = 0\n",
    "R = np.dot(D,D.T)                       ###-------->> Covariance matrix\n",
    "\n",
    "#4d) ----->> eigen vectors and eigen values using linalg function from python\n",
    "\n",
    "w , v = np.linalg.eigh(R)\n",
    "\n",
    "#4e) ----->> The eigen values are sorted in a decrecient way. Each one explains a percentage of co-variance\n",
    "\n",
    "ind_creciente = np.argsort(w) \n",
    "ind_decre     = ind_creciente[::-1] \n",
    "Lambda        = w[ind_decre]         #< eigen values sorted \n",
    "E             = v[:,ind_decre]       #< eigen vectors.        \n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "5) Here, the pcs and eofs are computed from the matrix A, E and R. Remember R is a\n",
    "   concatenation of fields. Thus, if PCA is applied to 1, 2 o 3 fields, We need to\n",
    "   separate R in order to represent each field with its respective pcs and eofs\n",
    "   pcs : principal components, eofs : orthogonal function\n",
    "   Note pcs represent the space pattern along dip while eofs reflect the along strike pattern\n",
    "   Important : Matrix A and E will contain all PCs and EOFs. If we wanna observe the variance\n",
    "   in maps considering an specific mode k, we need to extract the specific PC and EOF\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "#5a) --> First case : one field is analized, \n",
    "if  c == 1:\n",
    "\n",
    "        E1 = E[0:nx,:]                    #------> Eofs are extracted from matrix E\n",
    "        A1 = np.dot(E1.T,D[0:nx,:])       #------> PCs are computed between eofs and original fields\n",
    "        \n",
    "        #5a2 Reconstruction of the field taking into account the k mode\n",
    "        E_field_1    = np.reshape(E1[:,k-1],(nx,1)) ; A_field_1 = np.reshape(A1[k-1,:],(1,ny))\n",
    "        R_variance_1 = np.dot(E_field_1,A_field_1)\n",
    "\n",
    "        R_variance = [R_variance_1] #---> Matrix containing the fields to be plotted\n",
    "\n",
    "        EOFS = np.hstack((E_field_1,np.ones((nx,1))))     #------> EOFs are concatenated in one matrix for each field\n",
    "        PCs  = np.vstack((A_field_1,np.ones((1,ny))))     #------> PCs are concatenated in one matrix for each field\n",
    "\n",
    "        #5a3 Reconstruction of the fields considering the contribution of k modes if k > 1\n",
    "        i = 0 ; R_variance_sum_1 = np.zeros((nx,ny))\n",
    "        E1_mode_k_sum = np.zeros((nx,1)) ; A1_mode_k_sum = np.zeros((1,ny))\n",
    "        for i in range(k):\n",
    "            i = i+1\n",
    "            E1_mode_k      = np.reshape(E1[:,i-1],(nx,1)) ; A1_mode_k      = np.reshape(A1[i-1,:],(1,ny))  \n",
    "            E1_mode_k_sum += np.reshape(E1[:,i-1],(nx,1)) ; A1_mode_k_sum += np.reshape(A1[i-1,:],(1,ny))    \n",
    "            R_variance_sum_1  += np.dot(E1_mode_k,A1_mode_k)\n",
    " \n",
    "        R_variance_sum = [R_variance_sum_1] #---> Matrix containing the fields to be plotted\n",
    "\n",
    "        EOFSum = np.hstack((E1_mode_k_sum,np.ones((nx,1)))) #------> EOFs suma modes re concatenated in one matrix for each field\n",
    "        PCsum  = np.vstack((A1_mode_k_sum,np.ones((1,ny)))) #------> PCs sum modes are concatenated in one ma)trix for each field\n",
    "\n",
    "         \n",
    "        \n",
    "#5b) --> Second case : two fields are analized\n",
    "if  c == 2:\n",
    "\n",
    "        E1 = E[0:nx,:] ; E2 = E[nx:2*nx]  #------> Eofs are extracted, two groups related to each field\n",
    "        A1 = np.dot(E1.T,D[0:nx,:]) \n",
    "        A2 = np.dot(E2.T,D[nx:2*nx,:])    #------> PCs are computed between eofs and original fields \n",
    "                        \n",
    "        #5b2 Reconstruction of the field taking into account the k mode for the two fields\n",
    "        E_field_1    = np.reshape(E1[:,k-1],(nx,1)) ; A_field_1 = np.reshape(A1[k-1,:],(1,ny))\n",
    "        R_variance_1 = np.dot(E_field_1,A_field_1)\n",
    "        \n",
    "        E_field_2    = np.reshape(E2[:,k-1],(nx,1)) ; A_field_2 = np.reshape(A2[k-1,:],(1,ny))\n",
    "        R_variance_2 = np.dot(E_field_2,A_field_2)\n",
    "        \n",
    "        R_variance = [R_variance_1,R_variance_2] #---> Matrix containing the fields to be plotted\n",
    "\n",
    "        EOFS = np.hstack((E_field_1,E_field_2)) #------> EOFs are concatenated in one matrix for each field\n",
    "        PCs  = np.vstack((A_field_1,A_field_2)) #------> PCs are concatenated in one matrix for each field\n",
    "\n",
    "        #5b3 Reconstruction of the fields considering the contribution of k modes if k > 1\n",
    "        i = 0 ; R_variance_sum_1 = np.zeros((nx,ny)) ; R_variance_sum_2 = np.zeros((nx,ny))\n",
    "        E1_mode_k_sum = np.zeros((nx,1)) ; A1_mode_k_sum = np.zeros((1,ny))\n",
    "        E2_mode_k_sum = np.zeros((nx,1)) ; A2_mode_k_sum = np.zeros((1,ny))\n",
    "\n",
    "        for i in range(k):\n",
    "            i = i+1\n",
    "            E1_mode_k       = np.reshape(E1[:,i-1],(nx,1)) ; A1_mode_k      = np.reshape(A1[i-1,:],(1,ny))    \n",
    "            E1_mode_k_sum  += np.reshape(E1[:,i-1],(nx,1)) ; A1_mode_k_sum += np.reshape(A1[i-1,:],(1,ny))    \n",
    "            R_variance_sum_1  += np.dot(E1_mode_k,A1_mode_k)\n",
    "            \n",
    "            E2_mode_k       = np.reshape(E2[:,i-1],(nx,1)) ; A2_mode_k      = np.reshape(A2[i-1,:],(1,ny)) \n",
    "            E2_mode_k_sum  += np.reshape(E2[:,i-1],(nx,1)) ; A2_mode_k_sum += np.reshape(A2[i-1,:],(1,ny))    \n",
    "            R_variance_sum_2  += np.dot(E2_mode_k,A2_mode_k)\n",
    "            \n",
    "        R_variance_sum = [R_variance_sum_1,R_variance_sum_2] #---> Matrix containing the fields to be plotted\n",
    "\n",
    "        EOFSum = np.hstack((E1_mode_k_sum,E2_mode_k_sum)) #------> EOFs suma modes re concatenated in one matrix for each field\n",
    "        PCsum  = np.vstack((A1_mode_k_sum,A2_mode_k_sum)) #------> PCs sum modes are concatenated in one matrix for each field\n",
    "\n",
    "  \n",
    "  \n",
    "        \n",
    "#5c) --> Third case : three fields are analized\n",
    "if  c == 3:\n",
    "\n",
    "        E1 = E[0:nx,:] ; E2 = E[nx:2*nx] ; E3 = E[2*nx:3*nx]  #------> Eofs are extracted, two groups \n",
    "                                                              #        related to each field\n",
    "        A1 = np.dot(E1.T,D[0:nx,:]) \n",
    "        A2 = np.dot(E2.T,D[nx:2*nx,:])    \n",
    "        A3 = np.dot(E3.T,D[2*nx:3*nx,:])   #------> PCs are computed between eofs and original fields\n",
    "        \n",
    "        #5c2 Reconstruction of the field taking into account the k mode for the three fields\n",
    "        E_field_1    = np.reshape(E1[:,k-1],(nx,1)) ; A_field_1 = np.reshape(A1[k-1,:],(1,ny))\n",
    "        R_variance_1 = np.dot(E_field_1,A_field_1)\n",
    "        \n",
    "        E_field_2    = np.reshape(E2[:,k-1],(nx,1)) ; A_field_2 = np.reshape(A2[k-1,:],(1,ny))\n",
    "        R_variance_2 = np.dot(E_field_2,A_field_2)\n",
    "        \n",
    "        E_field_3    = np.reshape(E3[:,k-1],(nx,1)) ; A_field_3 = np.reshape(A3[k-1,:],(1,ny))\n",
    "        R_variance_3 = np.dot(E_field_3,A_field_3)\n",
    "        \n",
    "        R_variance = [R_variance_1,R_variance_2,R_variance_3] #---> Matrix containing the fields to be plotted\n",
    "        \n",
    "        EOFS = np.hstack((E_field_1,E_field_2,E_field_3)) #------> EOFs are concatenated in one matrix for each field\n",
    "        PCs  = np.vstack((A_field_1,A_field_2,A_field_3)) #------> PCs are concatenated in one matrix for each field\n",
    "\n",
    "        \n",
    "        #5c3 Reconstruction of the fields considering the contribution of k modes if k > 1\n",
    "        i = 0 ; R_variance_sum_1 = np.zeros((nx,ny)) ; R_variance_sum_2 = np.zeros((nx,ny)) ; R_variance_sum_3 = np.zeros((nx,ny))\n",
    "        E1_mode_k_sum = np.zeros((nx,1)) ; A1_mode_k_sum = np.zeros((1,ny))\n",
    "        E2_mode_k_sum = np.zeros((nx,1)) ; A2_mode_k_sum = np.zeros((1,ny))\n",
    "        E3_mode_k_sum = np.zeros((nx,1)) ; A3_mode_k_sum = np.zeros((1,ny))\n",
    "\n",
    "        for i in range(k):\n",
    "            i = i+1\n",
    "            E1_mode_k       = np.reshape(E1[:,i-1],(nx,1)) ; A1_mode_k = np.reshape(A1[i-1,:],(1,ny)) \n",
    "            E1_mode_k_sum  += np.reshape(E1[:,i-1],(nx,1)) ; A1_mode_k_sum += np.reshape(A1[i-1,:],(1,ny))    \n",
    "            R_variance_sum_1  += np.dot(E1_mode_k,A1_mode_k)\n",
    "            \n",
    "            E2_mode_k       = np.reshape(E2[:,i-1],(nx,1)) ; A2_mode_k = np.reshape(A2[i-1,:],(1,ny)) \n",
    "            E2_mode_k_sum  += np.reshape(E2[:,i-1],(nx,1)) ; A2_mode_k_sum += np.reshape(A2[i-1,:],(1,ny))    \n",
    "            R_variance_sum_2  += np.dot(E2_mode_k,A2_mode_k)\n",
    "            \n",
    "            E3_mode_k       = np.reshape(E3[:,i-1],(nx,1)) ; A3_mode_k      = np.reshape(A3[i-1,:],(1,ny)) \n",
    "            E3_mode_k_sum  += np.reshape(E3[:,i-1],(nx,1)) ; A3_mode_k_sum += np.reshape(A3[i-1,:],(1,ny))    \n",
    "            R_variance_sum_3  += np.dot(E3_mode_k,A3_mode_k)\n",
    " \n",
    "        R_variance_sum = [R_variance_sum_1,R_variance_sum_2,R_variance_sum_3] #---> Matrix containing the fields to be plotted\n",
    "\n",
    " \n",
    "        EOFSum = np.hstack((E1_mode_k_sum,E2_mode_k_sum,E3_mode_k_sum)) #------> EOFs suma modes re concatenated in one matrix for each field\n",
    "        PCsum  = np.vstack((A1_mode_k_sum,A2_mode_k_sum,A3_mode_k_sum)) #------> PCs sum modes are concatenated in one matrix for each field\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------------------------------------------------------------\n",
    "Step 5. In the following 4 cells, the plots are made. plot_function_pca is used and imported to this file\n",
    "        1) Reconstructed fields taking into account the k mode\n",
    "        2) Reconstructed fields taking into account the k modes (cumulative variance)\n",
    "        2) Display of the principal components (PCs)\n",
    "        3) Display of the empirical orthogonal functions (EOFs)\n",
    "        4) Explained variance by each mode plotted\n",
    "-------------------------------------------------------------------------------\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder   = ['Gravity', 'Locking', 'Friction']    \n",
    "fields = name_fields\n",
    "\n",
    "if int(c) == 1: \n",
    "    h = fields[0]\n",
    "    p = folder[h-1]\n",
    "\n",
    "if int(c) == 2:\n",
    "\n",
    "    if sum(fields) == 3: \n",
    "\n",
    "        p = '/'+folder[0]+'_'+folder[1]+'/'    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    if sum(fields) == 4:\n",
    "\n",
    "        p = '/'+folder[0]+'_'+folder[2]+'/'    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    if sum(fields) == 5:\n",
    "\n",
    "        p = '/'+folder[1]+'_'+folder[2]+'/'                \n",
    "\n",
    "if int(c) == 3:\n",
    "\n",
    "    p = '/'+folder[0]+'_'+folder[1]+'_'+folder[2]+'/'  \n",
    "#creating the folders to save the outputs\n",
    "path_fig_output_folder = path_work+'/fig_outputs/PCA_plots/ppf_'+str(largo/1000.)+'km/'+str(int(c))+'/'+p+'/'\n",
    "print path_fig_output_folder\n",
    "if os.path.isdir(path_fig_output_folder):\n",
    "    print 'Directory already existes'\n",
    "if os.path.isdir(path_fig_output_folder) == False:\n",
    "    print 'Directory created'\n",
    "    os.makedirs(path_fig_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#5a) Plotting the reconstructed fields. \n",
    "pfp.plot_recontruction(R_variance,c,k,name_fields,nx,ny,xi,yi,path_fig_output,path_data_grids,largo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sum of the k modes, change R_variance by R_variance_sum, if K>1\n",
    "pfp.plot_recontruction_sum(R_variance_sum,c,k,name_fields,nx,ny,xi,yi,path_fig_output,path_data_grids,largo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5b) Displaying the principal components. If you considerer a k mode different than 1, the sum of them are showed\n",
    "%matplotlib notebook\n",
    "pfp.plot_pcs(c,k,dist_p,PCs,PCsum,name_fields,path_fig_output,largo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5c) Displaying the orthogonal function. If you considerer a k mode different than 1, the sum of them are showed\n",
    "%matplotlib notebook\n",
    "pfp.plot_eofs(c,k,yi[:,100],EOFS,EOFSum,name_fields,path_fig_output,largo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5d) Showing the explained variance and cumulative explained variance for the k modes\n",
    "pfp.explained_variance(c,k,sum(np.array([name_fields])),path_fig_output,Lambda,largo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
